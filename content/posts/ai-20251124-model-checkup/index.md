+++
title = '我开始给 AI 做“体检”：不是为了挑刺，是为了别被它骗'
slug = 'ai-model-checkup'
date = 2025-11-24T22:15:00+08:00
draft = false
tags = ['AI', '评测', '可靠性']
categories = ['AI']
summary = '模型的“自信”不等于正确。做一套小小的评测题库，比吵架更有效。'
toc = true
math = false
+++

大模型有一种很特别的气质：它说错的时候也很笃定。

一开始我会跟它较劲——你怎么能把这个概念搞反？你怎么能瞎编一个函数？后来我发现，跟模型吵架没意义。**更有效的办法是：给它做体检。**

## 体检不是大工程，从 20 个问题开始

我给自己建了一个小文件，里面只有 20 个问题，都是我工作里经常遇到的：

- “这个接口什么时候会超时？超时后重试策略是什么？”
- “某个流程如果失败，应该怎么补偿？”
- “某个规则有哪些例外？”

这些问题有两个特点：

1) 我知道正确答案在哪里（文档、代码、经验里）
2) 它们很容易被模型“说得很像对”

我每次换模型、换提示词、换检索策略，就跑一遍这 20 个问题。

## 我最看重的指标不是“对不对”，而是“会不会承认不知道”

你问模型一个它不确定的问题，它有三种表现：

1) 直接承认不知道，并告诉你需要什么信息
2) 先给出假设，再给验证方式
3) 一本正经地编答案

第一种是最好的，第二种也能用，第三种最危险。

所以我的体检题里会刻意放 2-3 个“陷阱题”：信息不足但看起来很像能答。看它会不会乱编。

## 体检之后，我开始做“输出约束”

如果模型容易乱编，我就给它加上约束：

- “如果不确定，请明确说不确定。”
- “引用来源，至少给出文件名/章节名/链接。”
- “不要给最终结论，先给你要验证的 3 个点。”

这些约束不酷，但很管用。就像给新司机加一个限速：不一定让你更快，但能少出事故。

## 体检让团队沟通变轻松

以前讨论“这个模型好不好用”，大家说的都是感受：

“感觉它挺聪明。”
“感觉它经常胡说。”

现在我们有一份小题库，讨论变成了事实：

“第 7 题它答错了，错在把 A 当成 B。”
“加上来源引用后，第 12 题不再乱编了。”

讨论不再靠吵，也不再靠印象。

## 最后：体检不是为了证明它不行

我从来不指望模型 100% 正确。我的目标是：**知道它在哪些场景靠谱，在哪些场景需要我把手放在方向盘上。**

AI 很像一个特别能说的朋友——你喜欢跟他聊天，但你不会把他讲的每句话都当真。

体检做多了，你会越来越清楚：哪些话能信，哪些话要查，哪些话只当成“灵感”。

有了这层边界，使用 AI 反而更轻松。

